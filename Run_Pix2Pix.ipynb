{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.6.8 64-bit",
      "language": "python",
      "name": "python36864bit83541234a61a433782f27299de680c1c"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Run Pix2Pix.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devShaurya/Summer-Intern/blob/master/Run_Pix2Pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny71zKUNJsA5",
        "colab_type": "text"
      },
      "source": [
        "##Instructions for preparing data:- \n",
        "1. **Put the input data** in one of the directory \"A\" or \"B\" and output data in other directory. Divide each directory \"A\" and \"B\" into test, train and val. Name of each file should be like \"1.bmp\" or \"1.jpg\" like format. \n",
        "2. **Run git clone command** if this repo https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix has not been cloned please clone it. >\n",
        "3. **Run  python command to run combine the data in A and B** To check options see the link https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#prepare-your-own-datasets-for-pix2pix \n",
        "4. **If files are not of same size** add  im_A=cv2.resize(im_A,(512,512)) and im_B=cv2.resize(im_B,(512,512)) lines at line no. 47 and 48 in combine_A_and_B.py file. Size is second argument in cv2.resize command, change it acc. to your use\n",
        "5. change paths in the commands according to yourselves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYQjTdnsJsA6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ./pix2pixcyclegan/datasets/combine_A_and_B.py --fold_A ./cubicasa5k/A --fold_B ./cubicasa5k/B --fold_AB ./cubicasa5k/AB --num_imgs 3934"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po6i1BxYJsBB",
        "colab_type": "text"
      },
      "source": [
        "## Instructions for running model:-\n",
        "1. **Run pip install commands** The ones just below. If anyother libraries are missing, just add !pip install {library_name}\n",
        "2. **Run git clone command** if this repo https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix has not been cloned please clone it.\n",
        "3. **Run pix2pix command** \n",
        "        a.  \"dataroot\" option sets data directory ( or where you have stored AB mapping).\n",
        "        b.  \"name\" option sets name of the directory where models and result images of epochs would be saved.\n",
        "        c.  \"checkpoints_dir\" option sets folder where the \"name\" folder is created\n",
        "        d.  \"model\" sets the model to be used.\n",
        "        e.  \"direction\" sets direction towards which model will run for generation like producing B like images from A. So \"direction\" would be AtoB\n",
        "        f.  \"display_id\" option is used to see the results after epochs. if no display screen is available like in AWS instance or Colab then set it to -1\n",
        "        g.  For other options checkout this link https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/tips.md#notes-on-colorization or open tips.md inside the docs folder of the cloned pix2pix directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwj1oK0zJsBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install visdom"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI5PvxHhJsBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install dominate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_mGT55aJsBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmrsERcBh7Ji",
        "colab_type": "text"
      },
      "source": [
        "## Training of pix2pix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKJnoRkeJsBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ./pix2pixcyclegan/train.py --dataroot ./cubicasa5k/AB/ --name PlanGenv4 --checkpoints_dir ./pix2pixcyclegan/checkpointsDir --model pix2pix --direction AtoB --display_id -1 --batch_size 2  --save_epoch_freq 5 --n_epochs 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG1cEsHMh4ef",
        "colab_type": "text"
      },
      "source": [
        "## Testing Pix2Pix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ntSlniEinLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ./pix2pixcyclegan/test.py --dataroot ./cubicasa5k/AB/ --direction AtoB --model pix2pix --name PlanGenv4"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}